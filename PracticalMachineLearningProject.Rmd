---
title: "Practical Machine Learning Course Project"
author: "Ouadie EL FAROUKI"
date: "05/01/2018"
output: html_document
---

```{r setup, include=FALSE, cache=TRUE}
library(knitr)
opts_chunk$set(echo = TRUE, cache = TRUE)
```

## I.Overview  
This document is the final report of the Peer Assessment project from Coursera' s course Practical Machine Learning, as part of the Specialization in Data Science by Jhons Hopkins University.  
It's a report that summarizes the different steps we've followed to build a prediction model on a training sample of data generated by some wearable devices during weight lifting exercises *(Check the Data background)* to predict the manner in which the exercise was performed, and test/compare different models on the remaining test data.  
**Key Words : ** *Training, Testing, Classification, Trees, Random Forest *   

## II.Data Background : Human Activity Recognition  
Human Activity Recognition -HAR- has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.  
HAR research has traditionally focused on discriminating between different activities, in other words, to predict "which" activity was performed at a specific point in time. The approach proposed for the Weight Lifting Exercises dataset used in this project is to investigate "how well" an activity was performed by the wearer. The Data was generated by accelerometers placed on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  

*More at : http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har *  

## III.Loading and preprocessing Data   
### A.Loading Data  
We've chosen to load data samples directly from the sources provided in the course project page. A first skim of the data sample shows that the character "#DIV/0!" is in fact a missing value, we've mentionned that in the initial loading.  
```{r loadData, cache=TRUE}
urlTrain<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training<-read.csv(urlTrain , na.strings = c("NA","",'#DIV/0!'))
dim(training)
```
The raw sample contains 160 columns, the last one (classe) being the response variable we want to predict using predictors among the left 159 variables.   

### B.Cleaning Data  
A general exploratory analysis of the training data reveals 3 remarkable observations :  
- Many variables contain a 'lot' of missing values  
- Some variables have a low variability (they are almost unchanging)  
- The first variables are identification variables  
These kind of variables won't serve for model building, thus they will be removed as a pre-processing step before modeling.  

```{r cleanData, dependson="loadData", cache=TRUE}
library(caret, verbose = FALSE)
# a) Removing variables with a missing values majority
NaThreshHold<-round((dim(training)[1]/2)) #50% of the variable observation are NAs
naCol<-c(1:dim(training)[2])[as.logical(colSums(is.na(training))>NaThreshHold)]
training<-training[,-naCol]
dim(training)
# b) Removing Near-zero variability variables
nrzv<-nearZeroVar(training, saveMetrics=T)
farZvarCols<-(1:dim(training)[2])[!nrzv$nzv]
training<-training[,farZvarCols]
dim(training)
# c) Removing identification variables
training<-training[,-c(1:5)]       #ID Variables : "X", "user_name",  "raw_timestamp_part_1", "raw_timestamp_part_2","cvtd_timestamp"
dim(training)

```

The cleaning step ends up with a set containing only the 53 numerical variables of interest along with the categorical response variable *'classe'*.  

### C.Data Splitting  

In order to evaluate the accuracy of the different models we're going to build, the original training data is split randomly with 70% of the data for a training sample and 30% of the remaining used for testing.  
```{r dataSplitting, dependson="cleanData", cache=TRUE}
library(caret)
library(ggplot2)
set.seed(1995)  #Stting a seed for reproducibility
inTrain<-createDataPartition(training$classe, p=0.7,list = FALSE)
trainData<-training[inTrain,]
testData<-training[-inTrain,]
dim(trainData)
```
## IV.Prediction Model Building  
Since the reponse variable *'classe'* is categorical ($classe \in ('A','B','C','D','E')$), the problem is about a supervised learning method : **Classification** . We propose 3 different methods that belong to the familly of *Tree-Based Algorithms'* : 1-Decision Tree ; 2-Bagging Method ; 3-Random Forest.   

### A.Decision Tree   

Model fitting using *rpart()* function from *'rpart'* package :  
```{r decisionTree, dependson="dataSplitting", fig.height=7, fig.width=10, cache=TRUE}
library(rpart)
modelTree<-rpart(data = trainData, classe~., method = "class")
plot(modelTree, uniform=TRUE, main="Classification Tree using Decision Tree")
text(modelTree, pretty = 0, cex=0.7)
```
  
Predicting classes of the test Data and evaluating the model's accuracy :  
```{r decisionTreePred, dependson="decisionTree", cache=TRUE}
predTree<-predict(modelTree, newdata = testData, type = "class")
confMatrixTree<-confusionMatrix(predTree, testData$classe)
confMatrixTree$table
confMatrixTree$overall

```

### B.Bagging Model   

Bagging is a special case of random forest where number of variables used in each iteration is equal to the number of all predictors. We have chosen the *randomForest()* function from *'randomForest'* package, setting *mtry=53* which is the number of predictors in our case.  
```{r bagging, dependson="dataSplitting", cache=TRUE}
library(randomForest, verbose = FALSE)
modelbagg<-randomForest(data=trainData, classe~., mtry=53, importance=T)
modelbagg
varImpPlot(modelbagg, pch=9, cex=0.8, main = "Important Variables according to the Bagging Model")
```

Predicting classes of the test Data and evaluating the model's accuracy :  
```{r baggingPred, dependson="bagging", cache=TRUE}
predBagg<-predict(modelbagg, newdata = testData, type = "class")
confMatrixBagg<-confusionMatrix(predBagg, testData$classe)
confMatrixBagg$table
confMatrixBagg$overall
```

### C.Random Forest   

The commonly used number of bootsrapping predictors in the random forest method is equal to sqrt(p) (p being the size of all available predictors), in our case $\sqrt54\simeq8$  :  
```{r randomForest, dependson="dataSplitting", cache=TRUE}
library(randomForest, verbose = FALSE)
modelRf<-randomForest(data=trainData, classe~., mtry=8, importance=T)
modelRf
varImpPlot(modelRf, pch=9, cex=0.8, main = "Important Variables according to the Random Forest Model")
```

Predicting classes of the test Data and evaluating the model's accuracy :  

```{r randomForestPred, dependson="randomForest", cache=TRUE}
predRf<-predict(modelRf, newdata = testData, type = "class")
confMatrixRf<-confusionMatrix(predRf, testData$classe)
confMatrixRf$table
confMatrixRf$overall
```

## V.Conclusion  
The accuracies of the 3 models above can be summurized as follow:  
- Decision Tree : 7.247239e-01  
- Bagging :  0.9960918  
- Random Forest : 0.9977910  
The random Forest model's accuracy is slightly close to the bagging's one, and far better than the Decision Tree's.  Thus, the Random Forest is the 'winning' model, we can apply it to our Test data to predict the classes of the 20 observations :  


```{r finalTest, dependson="randomForest"}
urlTest<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
testing<-read.csv(urlTest , na.strings = c("NA","",'#DIV/0!'))
predictions<-predict(modelRf, newdata = testing, type = "class")
predictions
```

